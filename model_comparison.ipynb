{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bea7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "from typing import Dict, List\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
    "from sklearn.cluster import KMeans\n",
    "from bertopic import BERTopic\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8673f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelComparison:\n",
    "  def __init__(self, input_file=\"processed_data.json\", output_dir=\"model_comparison\"):\n",
    "    self.input_file = input_file\n",
    "    self.output_dir = output_dir\n",
    "    self.viz_dir = os.path.join(output_dir, \"visualizations\")\n",
    "    os.makedirs(self.viz_dir, exist_ok=True)\n",
    "    self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    self.keybert = KeyBERT(model=self.sentence_model)\n",
    "    \n",
    "  def load_data(self) -> Dict[str, List[str]]:\n",
    "    with open(self.input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    df = pd.DataFrame(data)\n",
    "    return {\n",
    "        'darkweb': df[df['source'] == 'darkweb']['text'].tolist(),\n",
    "        'reddit': df[df['source'] == 'reddit']['text'].tolist()\n",
    "    }\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af028aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "modelcompare = ModelComparison()\n",
    "data = modelcompare.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ec3e524",
   "metadata": {},
   "outputs": [],
   "source": [
    "darkweb_data = data['darkweb']\n",
    "reddit_data = data['reddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6772e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_BERTopic(texts, n_topics=5):\n",
    "  if not texts:\n",
    "    return None, [], [], None, None\n",
    "  model = BERTopic()\n",
    "  topics, probs = model.fit_transform(texts)\n",
    "  return model, topics, probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1a70b84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model, topics, probs = run_BERTopic(darkweb_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a3a184d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5096610903724429\n"
     ]
    }
   ],
   "source": [
    "from bertopic.representation import KeyBERTInspired\n",
    "\n",
    "# Fine-tune your topic representations\n",
    "representation_model = KeyBERTInspired()\n",
    "topic_model = BERTopic(nr_topics=\"auto\",representation_model=representation_model)\n",
    "topics, probs = topic_model.fit_transform(darkweb_data)\n",
    "\n",
    "\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "topic_model.save(\"model_comparison/best_models/BERTopic_darkweb\", serialization=\"pytorch\", save_ctfidf=True, save_embedding_model=embedding_model)\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "tokenizer = vectorizer.build_tokenizer()\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [tokenizer(doc) for doc in darkweb_data]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence = coherence_model.get_coherence()\n",
    "print(coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "43cbdce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5096610903724429\n"
     ]
    }
   ],
   "source": [
    "vectorizer = topic_model.vectorizer_model\n",
    "tokenizer = vectorizer.build_tokenizer()\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [tokenizer(doc) for doc in darkweb_data]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words_dark = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words_dark, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence = coherence_model.get_coherence()\n",
    "print(coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e9ebe540",
   "metadata": {},
   "outputs": [],
   "source": [
    "representation_model = KeyBERTInspired()\n",
    "topic_model = BERTopic(nr_topics=\"auto\",representation_model=representation_model)\n",
    "topics, probs = topic_model.fit_transform(reddit_data)\n",
    "\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "topic_model.save(\"model_comparison/best_models/BERTopic_reddit\", serialization=\"pytorch\", save_ctfidf=True, save_embedding_model=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5b814ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6232101023586243\n"
     ]
    }
   ],
   "source": [
    "vectorizer = topic_model.vectorizer_model\n",
    "tokenizer = vectorizer.build_tokenizer()\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [tokenizer(doc) for doc in reddit_data]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words_reddit = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words_reddit, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence = coherence_model.get_coherence()\n",
    "print(coherence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01c9770c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hacking',\n",
       " 'hacker',\n",
       " 'airplay',\n",
       " 'hacked',\n",
       " 'apple',\n",
       " 'vulnerability',\n",
       " 'wormable',\n",
       " 'leaked',\n",
       " 'breach',\n",
       " 'device']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_words_reddit[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "42683875",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['copyright',\n",
       " 'infringement',\n",
       " 'copyrighted',\n",
       " 'lawsuit',\n",
       " 'piracy',\n",
       " 'notice',\n",
       " 'claim',\n",
       " 'sue',\n",
       " 'content',\n",
       " 'court']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_words_dark[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5b9f2d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "model_st = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Embed the topic strings\n",
    "embeddings_reddit = model_st.encode(topic_words_reddit, normalize_embeddings=True)\n",
    "embeddings_darkweb = model_st.encode(topic_words_dark, normalize_embeddings=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "57949c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute cosine similarities between each Reddit and Darkweb topic\n",
    "similarity_matrix = cosine_similarity(embeddings_reddit, embeddings_darkweb)\n",
    "\n",
    "# For each Reddit topic, find the best-matching Darkweb topic\n",
    "threshold = 0.5  # Adjust this threshold as needed\n",
    "common_topics = []\n",
    "unique_reddit = []\n",
    "unique_darkweb = set(range(len(embeddings_darkweb)))  # Keep track of unmatched darkweb topics\n",
    "\n",
    "reddit_topic_ids = list(range(len(embeddings_reddit)))\n",
    "darkweb_topic_ids = list(range(len(embeddings_darkweb)))\n",
    "\n",
    "for i, reddit_topic in enumerate(reddit_topic_ids):\n",
    "    sims = similarity_matrix[i]\n",
    "    max_sim_idx = np.argmax(sims)\n",
    "    max_sim_val = sims[max_sim_idx]\n",
    "\n",
    "    if max_sim_val >= threshold:\n",
    "        darkweb_topic = darkweb_topic_ids[max_sim_idx]\n",
    "        common_topics.append((reddit_topic, darkweb_topic, max_sim_val))\n",
    "        unique_darkweb.discard(darkweb_topic)\n",
    "    else:\n",
    "        unique_reddit.append(reddit_topic)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bdf888e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Common Topics (Reddit ID, Darkweb ID, Similarity):\n",
      "3 <--> 1 | Sim: 0.72\n",
      "Reddit: ['tor', 'privacy', 'vpn', 'anonymity', 'surveillance', 'protect', 'torbased', 'onionbrowser', 'onion', 'traffic']\n",
      "Darkweb: ['enforcement', 'privacy', 'analytics', 'google', 'safeguard', 'data', 'security', 'information', 'garante', 'leaked']\n",
      "----\n",
      "7 <--> 1 | Sim: 0.74\n",
      "Reddit: ['privacy', 'privacyrelated', 'privacyguides', 'privacytools', 'privacytoolsio', 'freedom', 'cryptopartybln', 'intersection', 'transparent', 'technology']\n",
      "Darkweb: ['enforcement', 'privacy', 'analytics', 'google', 'safeguard', 'data', 'security', 'information', 'garante', 'leaked']\n",
      "----\n",
      "9 <--> 2 | Sim: 0.76\n",
      "Reddit: ['pirate', 'domain', 'torrent', 'thepiratebay', 'redirect', 'qbittorrent', 'tpb', 'website', 'bay', 'btguard']\n",
      "Darkweb: ['proxy', 'piratebay', 'pirate', 'haproxy', 'thepiratebay', 'cloudflare', 'bay', 'load', 'website', 'cache']\n",
      "----\n",
      "11 <--> 13 | Sim: 0.64\n",
      "Reddit: ['linux', 'linux4noobs', 'linuxmemes', 'gnu', 'linuxquestions', 'debian', 'kernel', 'software', 'mobile', 'operating']\n",
      "Darkweb: ['linux', 'distro', 'programmed', 'programmer', 'window', 'program', 'firefox', 'computing', 'accessibility', 'computer']\n",
      "----\n",
      "\n",
      "Unique Reddit Topics:\n",
      "0: ['hacking', 'hacker', 'airplay', 'hacked', 'apple', 'vulnerability', 'wormable', 'leaked', 'breach', 'device']\n",
      "1: ['affiliate', 'spam', 'moderator', 'moderated', 'banning', 'abusive', 'reviewed', 'review', 'userfriendly', 'neutral']\n",
      "2: ['security', 'enterprise', 'guideline', 'community', 'information', 'organization', 'check', 'question', 'rule', 'abide']\n",
      "4: ['darknet', 'shadowban', 'ban', 'spammer', 'network', 'anonymized', 'moderation', 'service', 'sitewide', 'peertopeer']\n",
      "5: ['doom', 'het', 'allesverwoestende', 'slayer', 'nieuwe', 'nieuwste', 'alle', 'demonen', 'chaos', 'playstation']\n",
      "6: ['tail', 'linux', 'macos', 'privacy', 'privacyrelated', 'privacyfocused', 'mac', 'operating', 'anonymity', 'software']\n",
      "8: ['qubes', 'qubesos', 'cyberattack', 'opensource', 'software', 'security', 'privacy', 'securelyisolated', 'digital', 'environment']\n",
      "10: ['opsec', 'security', 'netsec', 'emsec', 'protecting', 'infosec', 'protect', 'enforcement', 'operation', 'vulnerability']\n",
      "12: ['malware', 'antivirus', 'redditmalware', 'ransomware', 'tech', 'search', 'enthusiast', 'techsupport', 'anti', 'type']\n",
      "\n",
      "Unique Darkweb Topics:\n",
      "0: ['vpns', 'vpn', 'openvpn', 'blocklists', 'p2p', 'blocking', 'isp', 'security', 'block', 'privacy']\n",
      "3: ['privacy', 'analytics', 'apps', 'iphone', 'tracking', 'apple', 'personalization', 'app', 'data', 'fingerprinting']\n",
      "4: ['copyright', 'infringement', 'copyrighted', 'lawsuit', 'piracy', 'notice', 'claim', 'sue', 'content', 'court']\n",
      "5: ['crypto', '2024', 'happen', 'doomed', 'ending', 'credit', 'fraud', 'end', 'happened', 'finally']\n",
      "6: ['2025', '2024', 'banning', 'facebook', 'ban', 'tiktok', 'fascism', 'jan', 'lza', 'recent']\n",
      "7: ['encryption', 'criminal', 'outlawing', 'investigation', 'terrorism', 'gang', 'lawful', 'stargate38', 'public', 'device']\n",
      "8: ['kenya', 'passport', 'mpesa', 'sim', 'wallet', 'bank', 'cashing', 'mobile', 'coinbase', 'cashouts']\n",
      "9: ['dht', 'settlement', 'claim', 'extortionate', 'move', 'troll', 'racket', 'victim', 'notice', 'address']\n",
      "10: ['blocklists', 'vpn', 'secure', 'slow', 'sty', 'ineffective', 'using', 'way', '', '']\n",
      "11: ['proxybay', 'cached', 'offlinebay', 'proxy', 'contrail', 'date', 'dump', 'info', '2017', 'database']\n",
      "12: ['privacy', 'tribler', 'client', 'slow', 'support', 'seems', 'better', 'complete', 'quite', '']\n",
      "14: ['seedbox', 'seedboxes', 'tracker', 'upload', 'uploads', 'public', 'provider', 'site', 'host', 'looking']\n",
      "15: ['wifi', 'portforwarding', 'port', 'router', 'window', 'hide', 'change', 'mac', 'tracker', 'changing']\n",
      "16: ['telecommunication', 'telecom', 'provider', 'traffic', 'surveillance', 'ispai', 'enforcement', 'internet', 'monitoring', 'access']\n",
      "17: ['duckduckgo', 'search', 'searching', 'google', 'feature', 'privacy', 'access', 'chat', 'page', 'engine']\n",
      "18: ['lza', 'rza', 'gza', '2023', 'cdda', 'dec', '2013', 'wrote', 'destroyer', 'today']\n",
      "19: ['tracker', 'tracking', 'track', 'monitoring', 'surveillance', 'footstep', 'x265', 'h265', 'detect', 'view']\n",
      "20: ['surveillance', 'police', 'rcmp', 'hacking', 'wiretap', 'spyware', 'infiltrate', 'suspect', 'encrypted', 'criminal']\n",
      "21: ['ban', 'canada', 'effect', '2025', 'banned', 'benefit', 'illusive', 'wrote', 'american', 'working']\n",
      "22: ['github', 'git', 'tor', 'gitlab', 'oniongit', 'onion', 'gitonion', 'maskaw', 'announcements', 'website']\n",
      "23: ['youtube', 'peertube', 'torrent', 'vimeo', 'app', 'piracy', 'share', 'platform', 'content', 'video']\n",
      "24: ['review', 'site', 'thanks', 'choice', 'thank', 'helpful', 'wrote', 'awesome', 'lot', 'need']\n"
     ]
    }
   ],
   "source": [
    "print(\"Common Topics (Reddit ID, Darkweb ID, Similarity):\")\n",
    "for r, d, sim in common_topics:\n",
    "    print(f\"{r} <--> {d} | Sim: {sim:.2f}\")\n",
    "    print(f\"Reddit: {topic_words_reddit[r]}\")\n",
    "    print(f\"Darkweb: {topic_words_dark[d]}\")\n",
    "    print(\"----\")\n",
    "\n",
    "print(\"\\nUnique Reddit Topics:\")\n",
    "for r in unique_reddit:\n",
    "    print(f\"{r}: {topic_words_reddit[r]}\")\n",
    "\n",
    "print(\"\\nUnique Darkweb Topics:\")\n",
    "for d in unique_darkweb:\n",
    "    print(f\"{d}: {topic_words_dark[d]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c894a17",
   "metadata": {},
   "source": [
    "## Model comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "039db89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf90faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = [list(str.split()) for str in reddit_data]\n",
    "common_dictionary = Dictionary(new_data)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in new_data]\n",
    "model = LdaModel(common_corpus, id2word=common_dictionary)\n",
    "cm = CoherenceModel(model=model, texts=new_data, corpus=common_corpus, dictionary=common_dictionary, coherence='c_v')\n",
    "coherence = cm.get_coherence()  # get coherence value\n",
    "print(f\"Reddit Coherence Score: {coherence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a9e4fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_dictionary = Dictionary(new_data)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in new_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230f98f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LdaModel(common_corpus, id2word=common_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "fed2953c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4477480710029231"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cm = CoherenceModel(model=model, texts=new_data, corpus=common_corpus, dictionary=common_dictionary, coherence='c_v')\n",
    "coherence = cm.get_coherence()  # get coherence value\n",
    "coherence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
